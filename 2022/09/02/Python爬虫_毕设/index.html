<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Python 爬虫 / 毕设 | Qué miras Bobo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Python爬虫是利用Python编程语言编写的程序，用于自动化地从互联网上获取信息。爬虫可以访问网页、抓取数据，并将所需的信息提取出来，以便进行进一步的处理或分析。 反爬虫策略 设置合理的请求头。   123headers &#x3D; &#123;   # 伪装浏览器请求头   &#x27;Cookie&#x27;: &#x27;ispeed_lsm&#x3D;2; baikeVisitId&#x3D;b84d4a50-4">
<meta property="og:type" content="article">
<meta property="og:title" content="Python 爬虫 &#x2F; 毕设">
<meta property="og:url" content="http://example.com/2022/09/02/Python%E7%88%AC%E8%99%AB_%E6%AF%95%E8%AE%BE/index.html">
<meta property="og:site_name" content="Qué miras Bobo">
<meta property="og:description" content="Python爬虫是利用Python编程语言编写的程序，用于自动化地从互联网上获取信息。爬虫可以访问网页、抓取数据，并将所需的信息提取出来，以便进行进一步的处理或分析。 反爬虫策略 设置合理的请求头。   123headers &#x3D; &#123;   # 伪装浏览器请求头   &#x27;Cookie&#x27;: &#x27;ispeed_lsm&#x3D;2; baikeVisitId&#x3D;b84d4a50-4">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-09-02T03:11:00.000Z">
<meta property="article:modified_time" content="2024-03-19T05:56:57.838Z">
<meta property="article:author" content="fengcai">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Qué miras Bobo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="https://github.com/leo710aka/bk/blob/main/DT1.jpg?raw=true">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Qué miras Bobo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Python爬虫_毕设" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/09/02/Python%E7%88%AC%E8%99%AB_%E6%AF%95%E8%AE%BE/" class="article-date">
  <time class="dt-published" datetime="2022-09-02T03:11:00.000Z" itemprop="datePublished">2022-09-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Python 爬虫 / 毕设
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Python爬虫是利用Python编程语言编写的程序，用于自动化地从互联网上获取信息。爬虫可以访问网页、抓取数据，并将所需的信息提取出来，以便进行进一步的处理或分析。</p>
<h3 id="反爬虫策略"><a href="#反爬虫策略" class="headerlink" title="反爬虫策略"></a><strong>反爬虫策略</strong></h3><ul>
<li>设置合理的请求头。   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;   <span class="comment"># 伪装浏览器请求头</span></span><br><span class="line">   <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;ispeed_lsm=2; baikeVisitId=b84d4a50-436c-4e0f-9e29-dc2393e9cdca; COOKIE_SESSION=6_1_8_5_10_9_1_0_7_5_0_3_33002_0_2_0_1650022038_1650022034_1650022036%7C8%230_1_1650022030%7C1; BD_UPN=1126314751; BD_HOME=1; BD_CK_SAM=1; H_PS_645EC=89b2Pt9WoxiJHIC80g9QL3FIo7tdoc9Z9Gm9Nd6gkOPipOmTDtckrFlLxEpchFYkItCM; BAIDUID=FD56AC9125756B81A0E4EB7A60F27700:FG=1; BIDUPSID=FD56AC9125756B81E8CE802CC99B8074; PSTM=1648004100; BDUSS=Jpc2d4NGIwdzRCNVFTR0xNeS1IYXBLNTQwfjhzRnl3Z0xRSlZJTDhZeU1ibnhpRVFBQUFBJCQAAAAAAAAAAAEAAAAnQDHOyfq77rXDd2luZHkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIzhVGKM4VRiT2; H_PS_PSSID=36426_31660_35912_36167_34584_35979_36055_36235_26350; BA_HECTOR=ak20800k8kag8h8le71h8646s0q; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; PSINO=6&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36&#x27;</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>使用代理IP。</li>
<li>限制请求频率，避免被封IP。   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sleep_time = random.uniform(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">time.sleep(sleep_time)</span><br></pre></td></tr></table></figure></li>
<li>处理验证码和登录等复杂场景。</li>
</ul>
<h3 id="网络请求库："><a href="#网络请求库：" class="headerlink" title="网络请求库："></a><strong>网络请求库：</strong></h3><ul>
<li><code>requests</code>库：用于发送HTTP请求，获取网页内容。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># url_base = &#x27;http://leo/index.phtml?reportdate=&#123;year&#125;&amp;quarter=&#123;quarter&#125;&amp;p=&#123;page&#125;&#x27;</span></span><br><span class="line"><span class="comment"># url = url_base.format(year=iyear, quarter=iquarter, page=page)  # 替换url模板中的占位符</span></span><br><span class="line">response = requests.get(url=url, headers=self.headers)</span><br><span class="line">res = response.content.decode()  <span class="comment"># 获取服务器响应的内容，将其解码成字符串</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="HTML解析库："><a href="#HTML解析库：" class="headerlink" title="HTML解析库："></a><strong>HTML解析库：</strong></h3><ul>
<li><code>BeautifulSoup</code>：用于解析HTML文档，提取所需信息。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(res, <span class="string">&#x27;html.parser&#x27;</span>)           <span class="comment"># 解析 HTML 内容</span></span><br><span class="line">first_paragraph = soup.find(<span class="string">&#x27;p&#x27;</span>)                   <span class="comment"># 查找第一个&lt;p&gt;标签</span></span><br><span class="line">script = soup.find(<span class="built_in">id</span>=tag_id)                      <span class="comment"># 找到具有指定 id 属性的标签</span></span><br><span class="line"><span class="comment"># script.string.replace_with(new_string)           # 替换标签内容</span></span><br><span class="line"><span class="comment"># script.append(new_tag)                           # 在标签内追加新标签</span></span><br><span class="line"><span class="comment"># script.extract()                                 # 从文档中删除标签</span></span><br><span class="line">text = script.text                                 <span class="comment"># 获取标签内容   </span></span><br><span class="line"><span class="comment"># json_str = re.findall(r&#x27;\[.+\]&#x27;, text)[0]        # 用正则表达式 re 从 text 中查找一个或多个包含 JSON 数据的字符串的第一个匹配项</span></span><br><span class="line"><span class="comment"># data = json.loads(json_str)                      # 把json格式的字符串转换为Python类型</span></span><br><span class="line">script1 = soup.find(<span class="string">&#x27;p&#x27;</span>, class_=<span class="string">&#x27;paragraph&#x27;</span>)       <span class="comment"># 查找带有指定class的&lt;p&gt;标签</span></span><br><span class="line">script2 = soup.find(<span class="string">&#x27;a&#x27;</span>, href=<span class="string">&#x27;https://...&#x27;</span>)       <span class="comment"># 查找带有指定属性的&lt;a&gt;标签</span></span><br><span class="line">labels = soup.find_all(<span class="string">&#x27;a&#x27;</span>, attrs=&#123;<span class="string">&#x27;href&#x27;</span>: <span class="literal">True</span>&#125;)  <span class="comment"># 模糊搜索HTML代码中所有含href属性的&lt;a&gt;标签</span></span><br></pre></td></tr></table></figure></li>
<li><code>lxml</code>：使用类似 XPath 的功能解析HTML文档。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html</span><br><span class="line">tree = html.fromstring(html_doc)</span><br><span class="line">paragraphs = tree.xpath(<span class="string">&#x27;//p&#x27;</span>)                                 <span class="comment"># 使用 XPath 表达式查找&lt;p&gt;标签</span></span><br><span class="line">paragraphs_with_class = tree.xpath(<span class="string">&#x27;//p[@class=&quot;paragraph&quot;]&#x27;</span>)  <span class="comment"># 使用 XPath 表达式查找具有指定class属性值的&lt;p&gt;标签</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="数据存储："><a href="#数据存储：" class="headerlink" title="数据存储："></a><strong>数据存储：</strong></h3><ul>
<li>文件存储：将爬取的数据保存为文本文件、CSV文件等。</li>
<li>数据库：使用SQLite、MySQL、MongoDB等数据库存储数据。   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, path</span>):</span><br><span class="line">   <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:   <span class="comment"># 加载存储在JSON文件中的数据, 指定文件为UTF-8编码</span></span><br><span class="line">       data = json.load(fp)</span><br><span class="line">   <span class="keyword">return</span> data</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, data, path</span>):</span><br><span class="line">   <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:   <span class="comment"># 以json格式保存, 最近一日各国疫情数据</span></span><br><span class="line">       json.dump(data, fp, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">baidu_search</span>(<span class="params">v_result_file</span>):   <span class="comment"># 保存csv数据</span></span><br><span class="line">   df = pd.DataFrame(</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="string">&#x27;关键字&#x27;</span>: kw_list, <span class="string">&#x27;页码&#x27;</span>: page_list, <span class="string">&#x27;标题&#x27;</span>: title_list,  <span class="string">&#x27;百度链接&#x27;</span>: href_list, </span><br><span class="line">         <span class="string">&#x27;真实链接&#x27;</span>: real_url_list, <span class="string">&#x27;更新时间&#x27;</span>: time_list, <span class="string">&#x27;简介&#x27;</span>: desc_list,</span><br><span class="line">         <span class="string">&#x27;网站名称&#x27;</span>: site_list,</span><br><span class="line">      &#125;</span><br><span class="line">   )</span><br><span class="line">   <span class="keyword">if</span> os.path.exists(v_result_file):</span><br><span class="line">         header = <span class="literal">None</span>   <span class="comment"># 已存在的csv文件保存时不用加标头</span></span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">         <span class="comment"># 创建新文件时，设置csv文件标头</span></span><br><span class="line">         header = [<span class="string">&#x27;关键词&#x27;</span>, <span class="string">&#x27;页码&#x27;</span>, <span class="string">&#x27;标题&#x27;</span>, <span class="string">&#x27;百度链接&#x27;</span>, <span class="string">&#x27;真实链接&#x27;</span>, <span class="string">&#x27;更新时间&#x27;</span>, <span class="string">&#x27;简介&#x27;</span>, <span class="string">&#x27;网站名称&#x27;</span>] </span><br><span class="line">   df.to_csv(v_result_file, mode=<span class="string">&#x27;a+&#x27;</span>, index=<span class="literal">False</span>, header=header, encoding=<span class="string">&#x27;utf_8_sig&#x27;</span>)</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;结果保存成功:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(v_result_file))</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="爬虫框架"><a href="#爬虫框架" class="headerlink" title="爬虫框架"></a><strong>爬虫框架</strong></h3><ul>
<li>Scrapy：一个强大的Python爬虫框架，提供了高级功能如异步处理、中间件等。   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myproject</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="动态网页爬取"><a href="#动态网页爬取" class="headerlink" title="动态网页爬取"></a><strong>动态网页爬取</strong></h3><ul>
<li>使用Selenium或Headless浏览器模拟浏览器行为。</li>
<li>处理JavaScript渲染的页面。</li>
</ul>
<hr>

<h1 id="毕业设计"><a href="#毕业设计" class="headerlink" title="毕业设计"></a>毕业设计</h1><p><strong>选题</strong>：基于内地与香港金融平台的在线量化投资的数据原型系统的设计与开发<br>应完成的项目：<br>（0） 进行金融平台和数据原型系统相关的论文学习<br>（1） 明确系统的功能需求和业务流程，调研各种技术的可行性和使用方法，进行详细设计<br>（2） 爬取金融数据，建设数据库<br>（3） 开发系统前端界面和功能接口<br>（4） 利用Web开发框架及相关组件进行后端开发<br>（5） 对系统前后端功能进行优化和测试评价<br>（6） 完成论文的撰写以及答辩的准备 </p>
<h2 id="数据库建设"><a href="#数据库建设" class="headerlink" title="数据库建设"></a>数据库建设</h2><ul>
<li>香港市场股票数据库的构建<ul>
<li>MySql数据库：股票、基金、期货</li>
<li><h2 id="数据库表结构：参考旧论文"><a href="#数据库表结构：参考旧论文" class="headerlink" title="数据库表结构：参考旧论文"></a>数据库表结构：参考旧论文</h2></li>
</ul>
</li>
<li>用python程序爬取数据<ul>
<li>python爬虫</li>
<li>数据清洗</li>
<li>数据需要爬取相关历史数据，如果不能获取历史数据，就要从现在开始爬取每日的交易数据写入到数据库中；</li>
</ul>
</li>
<li>系统架构<ul>
<li>前端</li>
<li>后端：Django</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/09/02/Python%E7%88%AC%E8%99%AB_%E6%AF%95%E8%AE%BE/" data-id="clu74xu6l000heov54n8o0lc1" data-title="Python 爬虫 / 毕设" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/04/01/hexo-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          hexo | Hello World
        
      </div>
    </a>
  
  
    <a href="/2022/09/01/Python%E5%9F%BA%E7%A1%80/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Python 基础</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2046/12/">December 2046</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2046/12/30/nice-photo/">Nice Photo</a>
          </li>
        
          <li>
            <a href="/2024/01/11/RabbitMQ/">RabbitMQ</a>
          </li>
        
          <li>
            <a href="/2024/01/07/SpringCloud/">SpringCloud</a>
          </li>
        
          <li>
            <a href="/2024/01/04/%E9%AA%8F%E4%BC%AF/">骏 伯</a>
          </li>
        
          <li>
            <a href="/2023/10/20/%E6%8A%80%E8%83%BD/">寄 能</a>
          </li>
        
          <li>
            <a href="/2023/10/13/%E9%9D%A2%E8%AF%95_%E4%B8%AA%E4%BA%BA/">面 逝 | 个人</a>
          </li>
        
          <li>
            <a href="/2023/10/12/%E9%9D%A2%E8%AF%95_Java/">面 逝 | Java</a>
          </li>
        
          <li>
            <a href="/2023/10/11/%E9%9D%A2%E8%AF%95_%E7%BB%84%E4%BB%B6/">面 逝 | 组件</a>
          </li>
        
          <li>
            <a href="/2023/07/31/Colab/">Colab</a>
          </li>
        
          <li>
            <a href="/2023/07/14/Pytorch/">Pytorch</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 fengcai<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>